{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec93e42-6775-43e9-95bf-3040065e02a0",
   "metadata": {},
   "source": [
    "# NanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7eb3997-b3d7-42db-842d-32548bf76ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39554f26-c0b1-47af-b329-c6e9bc63db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dc2533-34c4-40fe-b41f-b225db608918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14169e23-f8b8-46d8-b4a7-0852e73d4925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703698f5-0655-477c-82ad-6096737b117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch: i for i, ch in enumerate(chars) }\n",
    "itos = { i: ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcc8427-d164-4c08-84d9-e83fd22a0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b143515-3e06-4f33-a174-17fc442a1f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b9c0f1f-fc12-4db2-94fa-1cd41dc96ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a63698-a00b-4e1d-9075-2f39a4ded183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # context length\n",
    "train_data[:block_size+1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf99df3-3aad-44c2-8d1a-fa29992efd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is: 47\n",
      "When input is tensor([18, 47]), the target is: 56\n",
      "When input is tensor([18, 47, 56]), the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# In a chunk of 9 characters, there are 8 individual training examples that get sent to the network.\n",
    "# This also helps the network to see different context sizes, from 1 to block_size. This allows the network to learn how to predict in multiple context lengths.\n",
    "# Let's take a look. \n",
    "X = train_data[:block_size]\n",
    "Y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = X[:t+1]\n",
    "    target = Y[t]\n",
    "    print(f\"When input is {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb100569-b3fb-4baa-bf97-e8863cf6e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---\n",
      "When input is tensor([24]), the target is: 43\n",
      "When input is tensor([24, 43]), the target is: 58\n",
      "When input is tensor([24, 43, 58]), the target is: 5\n",
      "When input is tensor([24, 43, 58,  5]), the target is: 57\n",
      "When input is tensor([24, 43, 58,  5, 57]), the target is: 1\n",
      "When input is tensor([24, 43, 58,  5, 57,  1]), the target is: 46\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46]), the target is: 43\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), the target is: 39\n",
      "When input is tensor([44]), the target is: 53\n",
      "When input is tensor([44, 53]), the target is: 56\n",
      "When input is tensor([44, 53, 56]), the target is: 1\n",
      "When input is tensor([44, 53, 56,  1]), the target is: 58\n",
      "When input is tensor([44, 53, 56,  1, 58]), the target is: 46\n",
      "When input is tensor([44, 53, 56,  1, 58, 46]), the target is: 39\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39]), the target is: 58\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), the target is: 1\n",
      "When input is tensor([52]), the target is: 58\n",
      "When input is tensor([52, 58]), the target is: 1\n",
      "When input is tensor([52, 58,  1]), the target is: 58\n",
      "When input is tensor([52, 58,  1, 58]), the target is: 46\n",
      "When input is tensor([52, 58,  1, 58, 46]), the target is: 39\n",
      "When input is tensor([52, 58,  1, 58, 46, 39]), the target is: 58\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58]), the target is: 1\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), the target is: 46\n",
      "When input is tensor([25]), the target is: 17\n",
      "When input is tensor([25, 17]), the target is: 27\n",
      "When input is tensor([25, 17, 27]), the target is: 10\n",
      "When input is tensor([25, 17, 27, 10]), the target is: 0\n",
      "When input is tensor([25, 17, 27, 10,  0]), the target is: 21\n",
      "When input is tensor([25, 17, 27, 10,  0, 21]), the target is: 1\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1]), the target is: 54\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), the target is: 39\n"
     ]
    }
   ],
   "source": [
    "# For efficiency purposes, we'll add an extra \"batch\" dimension to take advantage of GPU parallelism\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) -  block_size, (batch_size, )) # 4 numbers randomly generated between 0 and len(data) -  block_size\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"Inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print(\"---\")\n",
    "\n",
    "# The total number of examples here would be 8 x 4 = 32 independent examples, packed into a single batch x with targets y, which will all be simultaneously processed by the transformers\n",
    "for b in range(batch_size): # batch dim\n",
    "    for t in range(block_size): # time dim (context length)\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context}, the target is: {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72efe6-9408-4616-9e52-3c5329c9121f",
   "metadata": {},
   "source": [
    "## Simplest version: Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5f1a58-c2f1-477b-a4e3-e0145ca19070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "lfJeukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
      "lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers where B = Batch size (4) and T = Time/Context Size (8)\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) where C is channel/embedding dimension (vocab size, 65)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape the logits tensor so that it is (B*T, C) because PyTorch expects the channel dimension to be the second dimension\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        # Generate (B, T + 1), (B, T + 2), ... (B, T + max_new_tokens)\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # Becomes (B, C)\n",
    "            \n",
    "            # convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# First result is random if it's not trained\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad63888-e141-43a8-905a-e9a6f25d0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eded4a2-c865-4047-80a0-d8de0ba8dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.362440586090088\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c500a9-c1c8-4aa1-99d8-aa83f3954046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M:\n",
      "IUSh t,\n",
      "F th he d ke alved.\n",
      "Thupld, cipbll t\n",
      "I: ir w, l me sie hend lor ito'l an e\n",
      "\n",
      "I:\n",
      "Gochosen ea ar btamandd halind\n",
      "Aust, plt t wadyotl\n",
      "I bel qunganonoth he m he de avellis k'l, tond soran:\n",
      "\n",
      "WI he toust are bot g e n t s d je hid t his IAces I my ig t\n",
      "Ril'swoll e pupat inouleacends-athiqu heame\n"
     ]
    }
   ],
   "source": [
    "# This is a very simple model because we're only looking at the last character to see what the next character will be!\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b9e61-14f6-4f71-b760-8ffac43efb4e",
   "metadata": {},
   "source": [
    "## Self-attention: The mathematical trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28c42fc3-5ee2-4528-bee0-b14cc11134a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with a toy example\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd016f-64d8-44a9-8748-b3f42ec6da2d",
   "metadata": {},
   "source": [
    "### What do we want? We want tokens to talk to each other (couple-them)\n",
    "\n",
    "For example: The token at the 5th location should communicate to tokens in previous locations, and NOT from tokens in \"future\" locations.\n",
    "\n",
    "Information should only flow in one direction.\n",
    "\n",
    "How to do this? I'd like to average the features on tokens 1-5, which would represent a summary of the fifth token in the context of its history (tokens 1-4).\n",
    "\n",
    "A downside is that we'd lose a lot of information by compressing all the tokens like that but we'll worry about that later.\n",
    "\n",
    "For now, \n",
    "\n",
    "```\n",
    "for every batch:\n",
    "    for every t-th token in that sequence:\n",
    "        calculate the average of all the tokens up to the current one.\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a43ed-c45e-48f0-b5cc-4a49d100d521",
   "metadata": {},
   "source": [
    "#### Version 0: For-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ff3f99d-f4f7-4485-aa94-5a2fae2389eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want: x[b, t] = mean_{i <= t} x[b_i]\n",
    "xbow = torch.zeros((B, T, C)) # bow = bag of words, term to mean \"averaging\". I wish we would call this x_mean haha\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d127cb40-31ce-4e60-b6c0-e719382a3660",
   "metadata": {},
   "source": [
    "#### Version 1: Cool matrix multiplication trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29058cc5-e265-4ed5-a4e3-bb47d052a67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c=tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# but there's a more efficient way of doing this! Cool matrix multiplication trick :) \n",
    "# let's see it in an example\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# This generates a \"triangular\" matrix where the lower triangle (including the diagonal) contains ones and the upper one contains zeros\n",
    "# (Specifically, torch.tril zeros out the upper triangular part of the matrix)\n",
    "# Why is this useful? Consider the case where you multiply/dot product this matrix A (of shape m x n) by another matrix B (of shape n x k)\n",
    "# Every column in the resulting matrix will contain:\n",
    "# C[i, j] = A[i] @ B[j]\n",
    "# Which translated to our problem, it basically means we have added up all the values of column j of matrix B up to current row i (which would be \"t\")\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "print(f\"{a=}\")\n",
    "\n",
    "# and then we can divide it by the number of rows we're adding up (normalizing so all the rows add up to 1) :O :O genius\n",
    "# somehow this results in an average. I still need to internalize it but it's cool.\n",
    "# intuition: averaging ~= normalization? NO! \n",
    "# when we divide a sum of numbers by a constant C, that is the same as multiplying each of those numbers by 1/C!!!!!! LMAOOOO and each row in this triangular matrix contains the 1/C \"weight\" to multiply the list of numbers by. BAM!!\n",
    "# -> how much is each value contributing to the final average value... the \"weight\" of each number in matrix B is defined in matrix A!\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "print(f\"{a=}\")\n",
    "print(f\"---\")\n",
    "\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(f\"{b=}\")\n",
    "print(f\"---\")\n",
    "\n",
    "c = a @ b\n",
    "print(f\"{c=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58cdf416-c764-4eb2-97f0-bb6103ce26c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ok, then let's vectorize our operation: x[b, t] = mean_{i <= t} x[b_i]\n",
    "# REMEMBER, the goal here is that a token in the t-th position only gets information from all the tokens preceding it! \n",
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "print(f\"{weights=}\")\n",
    "\n",
    "\n",
    "# (T, T) @ (B, T, C) -- add a batch dim --> (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "xbow2 = weights @ x \n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a3b9b-b251-49d9-a65d-ec76c1408f2a",
   "metadata": {},
   "source": [
    "#### Version 2: Use the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49a2853f-ec41-419a-853b-be2048adf29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tril: \n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "Weights before masking: \n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Weights after masking: \n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Weights after softmax: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lastly, another way of doing this would be... THE SOFTMAX!!!!!!!\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# INTERPRETATION PT 1: Weights begin at 0, which represent interaction strength/affinity. How much of each token from the past do we want to aggregate/average up? \n",
    "#     At the beginning, all tokens can communicate with all tokens, but each of them has an \"affinity\" of zero to the current token\n",
    "weights = torch.zeros((T, T))\n",
    "\n",
    "# Make all the zeros in the upper triangular part of the matrix to be negative infinity because...\n",
    "print(f\"Tril: \\n{tril}\")\n",
    "print(f\"Weights before masking: \\n{weights}\")\n",
    "\n",
    "# INTERPRETATION PT 2: This is limiting communication to tokens from the past by making all tokens from the future of the current token to have an affinity of negative infinity (So we WON'T aggregate ANYTHING from those tokens).\n",
    "weights = weights.masked_fill(tril == 0, float(\"-inf\"))\n",
    "print(f\"Weights after masking: \\n{weights}\")\n",
    "\n",
    "# ... softmax will convert those to zero, and then it will normalize each row in the lower triangular part of the matrix :O :O :O \n",
    "# INTERPRETATION PT 3: This sets the amount of \"affinity\" that each token from the past will have when they get aggregated/averaged up, and for now each past token has an equal amount of \"affinity\" to the current token, which is 1/T (where T is the index of the current token)\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "print(f\"Weights after softmax: \\n{weights}\")\n",
    "\n",
    "# INTERPRETATION PT 4: Here is where we aggregate the tokens with the affinity matrix/weights, which will end up being a reflection of how \"interesting\" each token finds each other (for now, equally interesting)\n",
    "xbow3 = weights @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "\n",
    "\n",
    "# CONCLUSION: You can do weighted aggregation of your past elements by doing matrix multiplication of a lower-triangular fashion, where the elements of the lower triangular part of the matrix tell you how much of each element \"fuses\" into the current token/position\n",
    "# TRIPLE BAM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9566be-53d6-4f17-b9f0-a82770cf9362",
   "metadata": {},
   "source": [
    "#### Version 3: Another way, more nuanced: Self-attention\n",
    "\n",
    "What's our actual goal here? Grabbing information from the past... but in a data-dependent way - in other words, don't get information uniformly! Which is what we were doing earlier.\n",
    "\n",
    "\n",
    "This is what self-attention helps with :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfc6b4-cdbc-4abb-9abb-f881e7093345",
   "metadata": {},
   "source": [
    "**But how does self-attention do that?**\n",
    "\n",
    "Every single token at each position will emit 2 vectors:\n",
    "- _Query:_ What am I looking for?\n",
    "- _Key:_ What do I contain?\n",
    "\n",
    "\n",
    "*Then how do we get \"the affinity\"?*\n",
    "- By calculating the dot product between the query vector of the current token and key vectors of this and other tokens. This becomes `weights`. Interpretation - if 2 tokens \"align\" strongly, then we'll get to learn more about that other token as opposed to learning about other tokens equally.\n",
    "\n",
    "\n",
    "*For example*\n",
    "\n",
    "- Let's say that we're the 8th token, and we're a vowel looking for a consonant up to the 4th position. \n",
    "- Then we \"encode\" that information in our query vector. \n",
    "- In the key vectors that the other tokens emit, we could imagine one token \"encoding\" the fact that it is a consonant and that it comes before the 4th position, in one of the channels, as represented by a high number in that specific channel.\n",
    "- When we get the dot product between the query and key vectors for the 8th token and that other token, we'll see a high affinity represented by the \"high\" number resulting in the dot product, more than other keys from other tokens. In the weights vector here, we can see that the 8th token found the 4th token pretty interesting and therefore had a large affinity (`0.2297`) for it:\n",
    "```\n",
    "[0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]\n",
    "```\n",
    "- Ultimately, when the softmax comes, I'll end up including a lot of the information from that token in the aggregate of information.\n",
    "\n",
    "\n",
    "\n",
    "But what is this \"value\" vector we see? Think of the original X as the \"private\" information about a token, kept in X. The final vector:\n",
    "- _Value:_ What will I communicate to you, if you find me \"interesting\"?\n",
    "\n",
    "The value is what gets aggregated, not X!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dd3a47a-dbf6-4431-88f4-1c1f9cb222e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # Batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Time to build a single head that performs self-attention\n",
    "head_size = 16 # hyperparameter\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "weights = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float(\"-inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = weights @ v \n",
    "# out = weights @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ac05719-4dd2-4c9d-a25f-137c7b8e295b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da468b34-b620-4642-94d5-58a409ed3bec",
   "metadata": {},
   "source": [
    "### Notes on attention:\n",
    "\n",
    "**1. Attention is a communication mechanism**\n",
    "\n",
    "Given a set of nodes in a directed graph, where every node has a vector of information, and it gets to aggregate information via a weighted sum from all of the nodes that point to it - where the criteria for aggregation will be based on the data. In our example above, the graph would look like this: \n",
    "- The first token node has an edge to itself. \n",
    "- The second token node has an edge pointing to itself, and an edge from the first token node pointing to it. \n",
    "- The third node has an edge pointing to itself and 2 edges pointing to it: One coming from the first node and one from the second\n",
    "- And so on, until the 8th token node.\n",
    "\n",
    "But in principle, \"attention\" can be applied to any directed graph-like structure similar to this!\n",
    "\n",
    "**2. There is no notion of space**\n",
    "\n",
    "Attention acts over a set of vectors in the graph, without any knowledge of \"where\" each node is in the graph. This is why we use positional encodings! This is a crucial to other mechanisms like a convolution, where it's clear how the filter acts in \"space\".\n",
    "\n",
    "**3. Each example across batch dimension is of course processed completely independently and never \"talk\" to each other**\n",
    "\n",
    "The matrix multiplications happen independly of each other. For our directed graph analogy, this means we have 4 different pools of 8-node directed graphs that only talk to each other.\n",
    "\n",
    "**4. In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.**\n",
    "\n",
    "Future tokens don't talk to past tokens! But if you want to include information from the future and allow nodes to talk to each other (encoder), don't use the masking! \n",
    "- Encoder blocks: All nodes talk to each other. \n",
    "- Decoder blocks: Triangular masking to avoid \"looking at the answer\" when making a prediction!\n",
    "\n",
    "**5. \"self-attention\" just means that the keys, queries and values are produced from the same source X. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)**\n",
    "\n",
    "But in Encoder/Decoder transformers, Keys and Values can come from another source and queries come from X. So it's self-attention if we only look at our set of nodes, but it's cross-attention if we look at other nodes and grab information from them through the key/value vectors. The attention from our example is basically \"self-attention\".\n",
    "\n",
    "**6.\"Scaled\" attention additional divides `weights` by `1/sqrt(head_size)`. This makes it so when input `Q, K` are unit variance, `weights` will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below**\n",
    "\n",
    "\n",
    "- The variance of `weights` without scaling will be on the order of `head_size` (in our case, around 16)\n",
    "- But the variance of `weights` with scaling will be 1!\n",
    "\n",
    "This is important because `weights` get fed to softmax, and it is important that it stays \"diffuse\" (not very positive nor very negative numbers in it). *What happens if it contains very positive or very negative numbers?* **It will end up converging to one-hot vectors!**. Which means that every node will only be aggregating information from **a single node**, which is not ideal specially at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd6d0bce-34a6-47fd-8650-0f2292262ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) \n",
    "weights_with_scaling = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b16e03d-b19d-4d83-987a-1df2dc08d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k.var()=tensor(1.0966)\n",
      "q.var()=tensor(0.9416)\n",
      "weights.var()=tensor(16.1036)\n",
      "weights_with_scaling.var()=tensor(1.0065)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{k.var()=}\")\n",
    "print(f\"{q.var()=}\")\n",
    "print(f\"{weights.var()=}\")\n",
    "print(f\"{weights_with_scaling.var()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8668b12-68ea-4597-aaa3-d279da1364fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1000,  0.2000,  0.3000, -0.2000,  0.5000])\n",
      "torch.softmax(w1, dim=-1)=tensor([0.1799, 0.1988, 0.2197, 0.1333, 0.2684])\n"
     ]
    }
   ],
   "source": [
    "# Weights that are diffuse\n",
    "w1 = torch.tensor([0.1, 0.2, 0.3, -0.2, 0.5])\n",
    "print(f\"{w1}\")\n",
    "print(f\"{torch.softmax(w1, dim=-1)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0d297de-c2b8-40fa-9e97-027c45b0e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8000,  1.6000,  2.4000, -1.6000,  4.0000])\n",
      "torch.softmax(w2, dim=-1)=tensor([0.0305, 0.0678, 0.1510, 0.0028, 0.7479])\n"
     ]
    }
   ],
   "source": [
    "# Weights that are NOT diffuse and have large values\n",
    "w2 = w1*8\n",
    "print(f\"{w2}\")\n",
    "print(f\"{torch.softmax(w2, dim=-1)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "570cb5fe-f150-4f31-8e51-4e11a72beb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.2000,  6.4000,  9.6000, -6.4000, 16.0000])\n",
      "torch.softmax(w3, dim=-1)=tensor([2.7560e-06, 6.7612e-05, 1.6587e-03, 1.8666e-10, 9.9827e-01])\n"
     ]
    }
   ],
   "source": [
    "# Weights with even larger values\n",
    "w3 = w1*32\n",
    "print(f\"{w3}\")\n",
    "print(f\"{torch.softmax(w3, dim=-1)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd36898-3e9e-40a4-b9e3-6b3b5b36ebcb",
   "metadata": {},
   "source": [
    "### More optimizations to the transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4293d2-285a-4b98-94a3-47c1689c6e30",
   "metadata": {},
   "source": [
    "#### LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01740d5-01cb-4a5e-88c4-247c0bfc544f",
   "metadata": {},
   "source": [
    "First, recall that `BatchNorm` is a mechanism to ensure that any individual neuron had unit gaussian distribution (0 mean, 1 std output) across the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23b6f3c2-0c8f-49eb-970d-f8aee2ff7869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True) # batch mean\n",
    "            xvar = x.var(0, keepdim=True) # batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        # update the buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100) # Batch size: 32, 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "859e417c-934d-47c4-900e-ca985b4c9068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4901e-08), tensor(1.0000))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0].mean(), x[:, 0].std() # mean and std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a96b2e31-b8bb-46ae-94c2-50488f4431ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0411), tensor(1.0431))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :].mean(), x[0, :].std() # mean and std of the features of a single input from the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116c462-8e26-41f5-8b43-8c893b809b67",
   "metadata": {},
   "source": [
    "So, what's the difference between `BatchNorm` and `LayerNorm`? Well, instead of normalizing across all batches (the column dimension), we... normalize the row dimension: for every individual example, its 100-dimensional vector will be normalized. And that's pretty much it. LOL.\n",
    "\n",
    "This means the computation doesn't span across examples anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "021f5fbc-3e86-41aa-b894-ee3a777a79a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm(100)\n",
    "x = torch.randn(32, 100) # Batch size: 32, 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5123617d-2954-467d-8740-00b123918a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0].mean(), x[:, 0].std() # mean and std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82353869-30f6-4a74-8686-945fa7c01fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.5763e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, :].mean(), x[0, :].std() # mean and std of the features of a single input from the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f7fd0-716e-4b1e-99fc-5b999c3ef0e8",
   "metadata": {},
   "source": [
    "BAM!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381a8d2-c774-431b-9262-7b95c02ffb3e",
   "metadata": {},
   "source": [
    "## The Transformer and Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e24ee7-e96d-44f5-b902-7410c4f1cc14",
   "metadata": {},
   "source": [
    "**But why does the Attention Is All You need paper an Encoder + Decoder Transformer?**\n",
    "\n",
    "*Because we're concerned with a different task: Condition text generation on additional information other than itself, as opposed to an autoregressive model.*\n",
    "\n",
    "```\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rseaux de neurones sont gniaux! <START> neural networks are awesome!<END>\n",
    "```\n",
    "\n",
    "- The cross-atention component of this architecture is essential - the keys and values are coming from the encoder (french sentence), NOT just the decoder. The queries are still generated from X though!\n",
    "- Conditioning the decoding not just on the past of the current decoding but also on the fully encoded french sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afac33-41dc-4775-a1e1-92e2341eb041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
